{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916aba74",
   "metadata": {},
   "source": [
    "# This notebook is used to train all models for either deep learning and lgbm for all winner sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e4b68",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f375ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from placed_functions import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from lightgbm import plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581dd81a",
   "metadata": {},
   "source": [
    "List of month needed to be done : ['01', '02','03','05', '06','07', '08'] to have all 470 races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1342e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the month is 03\n",
      "shape of the train set : (6030, 1663)\n",
      "shape of the test set : (68, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (5949, 1618)\n",
      "shape of the y_train:  (5949, 14)\n",
      "shape of the X_test:  (68, 1618)\n",
      "shape of the y_test:  (68, 14)\n"
     ]
    }
   ],
   "source": [
    "all_month = ['01', '02','03','05', '06','07', '08']\n",
    "\n",
    "month = all_month[2]\n",
    "\n",
    "print(f'the month is {month}')\n",
    "display = True\n",
    "save_model = True\n",
    "A = prepare_and_split_data_placed(pd.read_hdf(f'../data/{month}_train_runners.h5','features'),\n",
    "                                pd.read_hdf(f'../data/{month}_test_runners.h5','features'))\n",
    "if A !=False :\n",
    "    X_train, y_train, X_test, y_test, y_train_value, y_test_value, X_test_init = A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0198a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfdcbeed",
   "metadata": {},
   "source": [
    "# We will just train for deel learning and will keep models winner from lgbm for prediction on placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10879a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all months we need to test so we train 7 times both models\n",
    "all_month = ['01', '02','03','05', '06','07', '08']\n",
    "\n",
    "\n",
    "for month in all_month : \n",
    "    print(f'We are trainning for this month: {month}')\n",
    "    save_model = False\n",
    "    \n",
    "    A = prepare_and_split_data_placed(pd.read_hdf(f'../data/{month}_train_runners.h5','features'),\n",
    "                                      pd.read_hdf(f'../data/{month}_test_runners.h5','features'))\n",
    "    \n",
    "    if A!= False:\n",
    "        X_train, y_train, X_test, y_test, y_train_value, y_test_value, X_test_init = A\n",
    "        \n",
    "    #### DEEP LEARNING\n",
    "    print(\"trainning deep learning\")\n",
    "        \n",
    "        \n",
    "    #hyperparameters for the deep learning model\n",
    "    num_neutron = 96\n",
    "    batch_size = 50\n",
    "    epoch= 40\n",
    "    \n",
    "    model = train_dl(num_neutron,batch_size,epoch,X_train,y_train,X_test,y_test)\n",
    "    \n",
    "\n",
    "    if save_model:\n",
    "        model.save(f'model/placed_DL_{month}.h5')\n",
    "        \n",
    "    else :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342e8f2",
   "metadata": {},
   "source": [
    "# With 40 epoch and 50 as a batch size, it takes 5min 40 to train all those sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f95ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02eddde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbccc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0cdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313a831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e425896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0b180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20380c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0c3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb74e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fde1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b650a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90c09de",
   "metadata": {},
   "source": [
    "# Depends if we train lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdca4f4",
   "metadata": {},
   "source": [
    "## Functions for LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36af74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the best paramaters for LGBM from the hyperopt optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bfe9234b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Score, Parameters]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-5d1d173b0d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#convert str to a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#best_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "parameters = pd.read_csv('results_hyperopt_placed.csv',header = 0, names=[\"Score\",\"Parameters\"]).sort_values(by=\"Score\",ascending = True)\n",
    "parameters.head(3)\n",
    "\n",
    "#convert str to a dict\n",
    "import ast\n",
    "best_params = ast.literal_eval(parameters.iloc[0].Parameters)\n",
    "#best_params\n",
    "\n",
    "#round values to only two after the coma for besr params\n",
    "for k, v in best_params.items():\n",
    "    if isinstance(best_params[k], float):\n",
    "        best_params[k] = round(v, 2)\n",
    "        \n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16823418",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'boosting': 'gbdt',\n",
    " 'feature_fraction': 0.6,\n",
    " 'learning_rate': 0.03,\n",
    " 'max_depth': 9,\n",
    " 'metric': 'multi_error',\n",
    " 'min_child_samples': 90,\n",
    " 'min_data_in_leaf': 99,\n",
    " 'n_estimators': 300,\n",
    " 'num_class': 14,\n",
    " 'num_leaves': 50,\n",
    " 'objective': 'multiclass',\n",
    " 'reg_alpha': 0.31,\n",
    " 'reg_lambda': 0.68,\n",
    " 'seed': 80,\n",
    " 'subsample': 0.88,\n",
    " 'verbose': -1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a1596",
   "metadata": {},
   "source": [
    "# Do the trainning for all sets and both model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73934067",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are trainning for this month: 01\n",
      "shape of the train set : (5878, 1663)\n",
      "shape of the test set : (83, 1663)\n",
      "hhere\n",
      "(83, 1664)\n",
      "(83, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (5798, 1618)\n",
      "shape of the y_train:  (5798, 14)\n",
      "shape of the X_test:  (83, 1618)\n",
      "shape of the y_test:  (83, 14)\n",
      "trainning deep learning\n",
      "Start training..\n",
      "\n",
      "Epoch 1/40\n",
      "112/116 [===========================>..] - ETA: 0s - loss: 9.8315 - precision: 0.4241WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fae843a7c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "116/116 [==============================] - 2s 9ms/step - loss: 9.8551 - precision: 0.4242 - val_loss: 12.5743 - val_precision: 0.2432\n",
      "Epoch 2/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 12.4933 - precision: 0.5154 - val_loss: 18.2084 - val_precision: 0.3214\n",
      "Epoch 3/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 17.8677 - precision: 0.5202 - val_loss: 23.3314 - val_precision: 0.2923\n",
      "Epoch 4/40\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 23.1124 - precision: 0.5014 - val_loss: 27.0295 - val_precision: 0.3662\n",
      "Epoch 5/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 28.9794 - precision: 0.4927 - val_loss: 29.7012 - val_precision: 0.3235\n",
      "Epoch 6/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 36.1688 - precision: 0.4819 - val_loss: 33.4323 - val_precision: 0.3243\n",
      "Epoch 7/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 43.0483 - precision: 0.4691 - val_loss: 35.9499 - val_precision: 0.3247\n",
      "Epoch 8/40\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 52.8944 - precision: 0.4459 - val_loss: 38.3591 - val_precision: 0.3544\n",
      "Epoch 9/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 63.7317 - precision: 0.4324 - val_loss: 44.0772 - val_precision: 0.4024\n",
      "Epoch 10/40\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 69.6924 - precision: 0.4412 - val_loss: 46.3030 - val_precision: 0.4026\n",
      "Epoch 11/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 79.5667 - precision: 0.4393 - val_loss: 49.0101 - val_precision: 0.3875\n",
      "Epoch 12/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 86.3563 - precision: 0.4306 - val_loss: 55.4289 - val_precision: 0.3544\n",
      "Epoch 13/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 102.9174 - precision: 0.4152 - val_loss: 62.8176 - val_precision: 0.3000\n",
      "Epoch 14/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 108.4063 - precision: 0.4289 - val_loss: 64.8704 - val_precision: 0.3827\n",
      "Epoch 15/40\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 116.1207 - precision: 0.4122 - val_loss: 64.2558 - val_precision: 0.3766\n",
      "Epoch 16/40\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 133.0432 - precision: 0.4120 - val_loss: 69.5544 - val_precision: 0.4125\n",
      "Epoch 17/40\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 133.3727 - precision: 0.4093 - val_loss: 74.7915 - val_precision: 0.4217\n",
      "Epoch 18/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 156.4635 - precision: 0.4056 - val_loss: 74.9452 - val_precision: 0.3500\n",
      "Epoch 19/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 170.7603 - precision: 0.4255 - val_loss: 81.3567 - val_precision: 0.3418\n",
      "Epoch 20/40\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 172.9954 - precision: 0.4136 - val_loss: 84.2352 - val_precision: 0.3924\n",
      "Epoch 21/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 182.7707 - precision: 0.4002 - val_loss: 90.8256 - val_precision: 0.4250\n",
      "Epoch 22/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 211.5832 - precision: 0.3977 - val_loss: 93.3553 - val_precision: 0.3780\n",
      "Epoch 23/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 202.4345 - precision: 0.4237 - val_loss: 96.5102 - val_precision: 0.4125\n",
      "Epoch 24/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 224.3725 - precision: 0.3908 - val_loss: 99.8263 - val_precision: 0.3500\n",
      "Epoch 25/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 244.7728 - precision: 0.3969 - val_loss: 106.1611 - val_precision: 0.3462\n",
      "Epoch 26/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 253.0113 - precision: 0.4017 - val_loss: 106.8124 - val_precision: 0.4304\n",
      "Epoch 27/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 259.4127 - precision: 0.4232 - val_loss: 115.2011 - val_precision: 0.3415\n",
      "Epoch 28/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 251.7127 - precision: 0.3998 - val_loss: 117.6339 - val_precision: 0.3614\n",
      "Epoch 29/40\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 282.7971 - precision: 0.4157 - val_loss: 123.0003 - val_precision: 0.3500\n",
      "Epoch 30/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 331.5542 - precision: 0.4021 - val_loss: 126.9501 - val_precision: 0.3580\n",
      "Epoch 31/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 282.2242 - precision: 0.4025 - val_loss: 129.9591 - val_precision: 0.3494\n",
      "Epoch 32/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 313.2492 - precision: 0.3958 - val_loss: 135.5867 - val_precision: 0.3210\n",
      "Epoch 33/40\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 353.3978 - precision: 0.3936 - val_loss: 140.0086 - val_precision: 0.4074\n",
      "Epoch 34/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 349.4425 - precision: 0.3854 - val_loss: 136.5836 - val_precision: 0.4512\n",
      "Epoch 35/40\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 349.4398 - precision: 0.3938 - val_loss: 149.8970 - val_precision: 0.3614\n",
      "Epoch 36/40\n",
      "116/116 [==============================] - 1s 6ms/step - loss: 389.5481 - precision: 0.4061 - val_loss: 148.4838 - val_precision: 0.3625\n",
      "Epoch 37/40\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 355.5467 - precision: 0.4034 - val_loss: 149.0437 - val_precision: 0.4024\n",
      "Epoch 38/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 428.3666 - precision: 0.4093 - val_loss: 156.2765 - val_precision: 0.3875\n",
      "Epoch 39/40\n",
      "116/116 [==============================] - 1s 7ms/step - loss: 432.6996 - precision: 0.3994 - val_loss: 161.5483 - val_precision: 0.3133\n",
      "Epoch 40/40\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 494.1863 - precision: 0.3963 - val_loss: 166.1273 - val_precision: 0.3000\n",
      "Done.\n",
      "trainning lgbm\n",
      "We are trainning for this month: 02\n",
      "shape of the train set : (5961, 1663)\n",
      "shape of the test set : (69, 1663)\n",
      "hhere\n",
      "(69, 1664)\n",
      "(68, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (5881, 1618)\n",
      "shape of the y_train:  (5881, 14)\n",
      "shape of the X_test:  (68, 1618)\n",
      "shape of the y_test:  (68, 14)\n",
      "trainning deep learning\n",
      "Start training..\n",
      "\n",
      "Epoch 1/40\n",
      "118/118 [==============================] - 2s 9ms/step - loss: 9.8543 - precision: 0.3555 - val_loss: 13.3970 - val_precision: 0.4138\n",
      "Epoch 2/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 13.6456 - precision: 0.4920 - val_loss: 20.5203 - val_precision: 0.3400\n",
      "Epoch 3/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 20.4313 - precision: 0.5049 - val_loss: 26.7411 - val_precision: 0.2909\n",
      "Epoch 4/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 27.5511 - precision: 0.4788 - val_loss: 29.1026 - val_precision: 0.3390\n",
      "Epoch 5/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 35.4853 - precision: 0.4765 - val_loss: 30.8756 - val_precision: 0.2833\n",
      "Epoch 6/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 41.5830 - precision: 0.4582 - val_loss: 33.7628 - val_precision: 0.2769\n",
      "Epoch 7/40\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 49.2238 - precision: 0.4532 - val_loss: 35.3280 - val_precision: 0.3438\n",
      "Epoch 8/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 60.0057 - precision: 0.4481 - val_loss: 38.2817 - val_precision: 0.3438\n",
      "Epoch 9/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 66.9711 - precision: 0.4286 - val_loss: 41.2259 - val_precision: 0.3385\n",
      "Epoch 10/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 78.0407 - precision: 0.4370 - val_loss: 44.9878 - val_precision: 0.3030\n",
      "Epoch 11/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 82.2353 - precision: 0.4348 - val_loss: 52.3959 - val_precision: 0.3235\n",
      "Epoch 12/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 92.5538 - precision: 0.4332 - val_loss: 52.9287 - val_precision: 0.3077\n",
      "Epoch 13/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 100.5002 - precision: 0.4199 - val_loss: 52.7311 - val_precision: 0.3175\n",
      "Epoch 14/40\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 108.8858 - precision: 0.4244 - val_loss: 53.9606 - val_precision: 0.3676\n",
      "Epoch 15/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 123.7724 - precision: 0.4291 - val_loss: 59.5292 - val_precision: 0.3636\n",
      "Epoch 16/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 143.0019 - precision: 0.4269 - val_loss: 58.1099 - val_precision: 0.3692\n",
      "Epoch 17/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 155.4213 - precision: 0.4333 - val_loss: 64.7072 - val_precision: 0.3030\n",
      "Epoch 18/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 156.3449 - precision: 0.4323 - val_loss: 67.0049 - val_precision: 0.3077\n",
      "Epoch 19/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 159.9225 - precision: 0.4199 - val_loss: 68.2455 - val_precision: 0.2923\n",
      "Epoch 20/40\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 183.1795 - precision: 0.4138 - val_loss: 72.2458 - val_precision: 0.3030\n",
      "Epoch 21/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 189.9319 - precision: 0.4132 - val_loss: 70.8275 - val_precision: 0.3030\n",
      "Epoch 22/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 186.7935 - precision: 0.4255 - val_loss: 75.6742 - val_precision: 0.2121\n",
      "Epoch 23/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 206.9236 - precision: 0.4206 - val_loss: 75.2408 - val_precision: 0.3231\n",
      "Epoch 24/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 221.3029 - precision: 0.4178 - val_loss: 81.2534 - val_precision: 0.3485\n",
      "Epoch 25/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 255.6219 - precision: 0.4204 - val_loss: 82.0290 - val_precision: 0.3333\n",
      "Epoch 26/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 267.6693 - precision: 0.4113 - val_loss: 83.7971 - val_precision: 0.3125\n",
      "Epoch 27/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 258.9550 - precision: 0.4110 - val_loss: 80.0497 - val_precision: 0.4179\n",
      "Epoch 28/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 270.9074 - precision: 0.4327 - val_loss: 86.2167 - val_precision: 0.3182\n",
      "Epoch 29/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 272.6145 - precision: 0.4309 - val_loss: 93.2990 - val_precision: 0.3333\n",
      "Epoch 30/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 301.9386 - precision: 0.4190 - val_loss: 95.6814 - val_precision: 0.3382\n",
      "Epoch 31/40\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 298.9396 - precision: 0.4130 - val_loss: 95.1225 - val_precision: 0.3182\n",
      "Epoch 32/40\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 299.1630 - precision: 0.3968 - val_loss: 100.9500 - val_precision: 0.2941\n",
      "Epoch 33/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 336.4293 - precision: 0.4114 - val_loss: 98.7897 - val_precision: 0.3846\n",
      "Epoch 34/40\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 315.9847 - precision: 0.4194 - val_loss: 103.1408 - val_precision: 0.3385\n",
      "Epoch 35/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 346.2700 - precision: 0.4083 - val_loss: 100.7013 - val_precision: 0.3284\n",
      "Epoch 36/40\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 344.8706 - precision: 0.4172 - val_loss: 102.5330 - val_precision: 0.3731\n",
      "Epoch 37/40\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 372.9544 - precision: 0.4213 - val_loss: 101.1745 - val_precision: 0.3582\n",
      "Epoch 38/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 407.5659 - precision: 0.3954 - val_loss: 110.9706 - val_precision: 0.3582\n",
      "Epoch 39/40\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 383.8418 - precision: 0.4064 - val_loss: 113.5749 - val_precision: 0.2941\n",
      "Epoch 40/40\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 440.1752 - precision: 0.4129 - val_loss: 114.4027 - val_precision: 0.3676\n",
      "Done.\n",
      "trainning lgbm\n",
      "We are trainning for this month: 03\n",
      "shape of the train set : (6030, 1663)\n",
      "shape of the test set : (68, 1663)\n",
      "hhere\n",
      "(68, 1664)\n",
      "(68, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (5949, 1618)\n",
      "shape of the y_train:  (5949, 14)\n",
      "shape of the X_test:  (68, 1618)\n",
      "shape of the y_test:  (68, 14)\n",
      "trainning deep learning\n",
      "Start training..\n",
      "\n",
      "Epoch 1/40\n",
      "119/119 [==============================] - 2s 11ms/step - loss: 9.8896 - precision: 0.3331 - val_loss: 11.9635 - val_precision: 0.3448\n",
      "Epoch 2/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 12.9172 - precision: 0.5226 - val_loss: 16.4130 - val_precision: 0.4490\n",
      "Epoch 3/40\n",
      "119/119 [==============================] - 1s 5ms/step - loss: 18.5361 - precision: 0.4736 - val_loss: 21.9594 - val_precision: 0.4231\n",
      "Epoch 4/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 24.8433 - precision: 0.4874 - val_loss: 27.3225 - val_precision: 0.3559\n",
      "Epoch 5/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 32.1112 - precision: 0.4784 - val_loss: 29.8792 - val_precision: 0.3667\n",
      "Epoch 6/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 39.5605 - precision: 0.4590 - val_loss: 33.2727 - val_precision: 0.3279\n",
      "Epoch 7/40\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 46.3106 - precision: 0.4429 - val_loss: 37.2691 - val_precision: 0.4000\n",
      "Epoch 8/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 55.7055 - precision: 0.4417 - val_loss: 39.4890 - val_precision: 0.3433\n",
      "Epoch 9/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 63.8521 - precision: 0.4391 - val_loss: 46.7707 - val_precision: 0.2857\n",
      "Epoch 10/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 76.4332 - precision: 0.4416 - val_loss: 47.2928 - val_precision: 0.3030\n",
      "Epoch 11/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 84.4903 - precision: 0.4247 - val_loss: 50.7350 - val_precision: 0.2667\n",
      "Epoch 12/40\n",
      "119/119 [==============================] - 1s 9ms/step - loss: 92.9542 - precision: 0.4416 - val_loss: 58.7228 - val_precision: 0.2794\n",
      "Epoch 13/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 103.5120 - precision: 0.4174 - val_loss: 58.1349 - val_precision: 0.3182\n",
      "Epoch 14/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 109.6455 - precision: 0.4242 - val_loss: 63.1658 - val_precision: 0.2881\n",
      "Epoch 15/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 116.3074 - precision: 0.4118 - val_loss: 66.3676 - val_precision: 0.2687\n",
      "Epoch 16/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 136.5755 - precision: 0.4226 - val_loss: 65.3723 - val_precision: 0.3594\n",
      "Epoch 17/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 141.6697 - precision: 0.4084 - val_loss: 72.6709 - val_precision: 0.2836\n",
      "Epoch 18/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 159.0712 - precision: 0.4263 - val_loss: 80.7787 - val_precision: 0.3333\n",
      "Epoch 19/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 173.5638 - precision: 0.4014 - val_loss: 77.8736 - val_precision: 0.3485\n",
      "Epoch 20/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 187.3798 - precision: 0.4100 - val_loss: 81.5022 - val_precision: 0.3387\n",
      "Epoch 21/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 195.0063 - precision: 0.3974 - val_loss: 91.2785 - val_precision: 0.3030\n",
      "Epoch 22/40\n",
      "119/119 [==============================] - 1s 5ms/step - loss: 207.6322 - precision: 0.4219 - val_loss: 89.1157 - val_precision: 0.3284\n",
      "Epoch 23/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 204.1366 - precision: 0.4051 - val_loss: 92.5164 - val_precision: 0.3906\n",
      "Epoch 24/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 239.7350 - precision: 0.4059 - val_loss: 97.8248 - val_precision: 0.3433\n",
      "Epoch 25/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 261.9159 - precision: 0.4093 - val_loss: 103.2316 - val_precision: 0.3088\n",
      "Epoch 26/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 262.4423 - precision: 0.4090 - val_loss: 104.1782 - val_precision: 0.4328\n",
      "Epoch 27/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 265.1985 - precision: 0.4122 - val_loss: 106.7446 - val_precision: 0.3582\n",
      "Epoch 28/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 274.1699 - precision: 0.4093 - val_loss: 108.9087 - val_precision: 0.3333\n",
      "Epoch 29/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 315.0565 - precision: 0.4223 - val_loss: 116.0159 - val_precision: 0.2941\n",
      "Epoch 30/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 343.5503 - precision: 0.4064 - val_loss: 113.3123 - val_precision: 0.3846\n",
      "Epoch 31/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 349.3811 - precision: 0.3975 - val_loss: 123.6808 - val_precision: 0.4091\n",
      "Epoch 32/40\n",
      "119/119 [==============================] - 1s 6ms/step - loss: 328.0708 - precision: 0.3938 - val_loss: 123.7827 - val_precision: 0.3182\n",
      "Epoch 33/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 382.5562 - precision: 0.4045 - val_loss: 129.5390 - val_precision: 0.3382\n",
      "Epoch 34/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 390.2361 - precision: 0.4045 - val_loss: 133.9198 - val_precision: 0.3134\n",
      "Epoch 35/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 393.6513 - precision: 0.3927 - val_loss: 142.8043 - val_precision: 0.2941\n",
      "Epoch 36/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 402.3545 - precision: 0.4033 - val_loss: 149.1265 - val_precision: 0.3676\n",
      "Epoch 37/40\n",
      "119/119 [==============================] - 1s 11ms/step - loss: 423.7506 - precision: 0.3882 - val_loss: 149.9149 - val_precision: 0.3433\n",
      "Epoch 38/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 427.9325 - precision: 0.4199 - val_loss: 151.0887 - val_precision: 0.4030\n",
      "Epoch 39/40\n",
      "119/119 [==============================] - 1s 7ms/step - loss: 454.7338 - precision: 0.4010 - val_loss: 154.8760 - val_precision: 0.2308\n",
      "Epoch 40/40\n",
      "119/119 [==============================] - 1s 8ms/step - loss: 480.6096 - precision: 0.3846 - val_loss: 150.4906 - val_precision: 0.3881\n",
      "Done.\n",
      "trainning lgbm\n",
      "We are trainning for this month: 05\n",
      "shape of the train set : (6098, 1663)\n",
      "shape of the test set : (20, 1663)\n",
      "hhere\n",
      "(20, 1664)\n",
      "(20, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (6017, 1618)\n",
      "shape of the y_train:  (6017, 14)\n",
      "shape of the X_test:  (20, 1618)\n",
      "shape of the y_test:  (20, 14)\n",
      "trainning deep learning\n",
      "Start training..\n",
      "\n",
      "Epoch 1/40\n",
      "121/121 [==============================] - 2s 9ms/step - loss: 10.0135 - precision: 0.3329 - val_loss: 12.6101 - val_precision: 0.5000\n",
      "Epoch 2/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 13.6120 - precision: 0.4938 - val_loss: 19.1759 - val_precision: 0.3333\n",
      "Epoch 3/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 18.6615 - precision: 0.4725 - val_loss: 27.0033 - val_precision: 0.3125\n",
      "Epoch 4/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 23.5993 - precision: 0.4939 - val_loss: 31.8810 - val_precision: 0.3333\n",
      "Epoch 5/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 31.1219 - precision: 0.4791 - val_loss: 39.8028 - val_precision: 0.3500\n",
      "Epoch 6/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 37.5856 - precision: 0.4728 - val_loss: 43.5359 - val_precision: 0.3889\n",
      "Epoch 7/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 43.9821 - precision: 0.4589 - val_loss: 49.5637 - val_precision: 0.3333\n",
      "Epoch 8/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 53.8767 - precision: 0.4359 - val_loss: 58.2876 - val_precision: 0.2353\n",
      "Epoch 9/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 62.7653 - precision: 0.4454 - val_loss: 68.9712 - val_precision: 0.3125\n",
      "Epoch 10/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 73.2979 - precision: 0.4398 - val_loss: 71.2058 - val_precision: 0.1579\n",
      "Epoch 11/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 82.6277 - precision: 0.4357 - val_loss: 82.4268 - val_precision: 0.2778\n",
      "Epoch 12/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 90.1592 - precision: 0.4230 - val_loss: 87.2498 - val_precision: 0.3000\n",
      "Epoch 13/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 101.6294 - precision: 0.4166 - val_loss: 101.7411 - val_precision: 0.3000\n",
      "Epoch 14/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 112.8995 - precision: 0.4266 - val_loss: 103.2193 - val_precision: 0.3158\n",
      "Epoch 15/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 116.0838 - precision: 0.4113 - val_loss: 114.0143 - val_precision: 0.2500\n",
      "Epoch 16/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 136.4668 - precision: 0.4123 - val_loss: 123.4463 - val_precision: 0.3000\n",
      "Epoch 17/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 146.7366 - precision: 0.4246 - val_loss: 138.0846 - val_precision: 0.2500\n",
      "Epoch 18/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 151.6507 - precision: 0.4111 - val_loss: 133.4122 - val_precision: 0.3500\n",
      "Epoch 19/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 166.8390 - precision: 0.4180 - val_loss: 153.8552 - val_precision: 0.2632\n",
      "Epoch 20/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 177.1329 - precision: 0.3979 - val_loss: 167.4653 - val_precision: 0.2632\n",
      "Epoch 21/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 199.4734 - precision: 0.4022 - val_loss: 174.3097 - val_precision: 0.1500\n",
      "Epoch 22/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 193.4194 - precision: 0.4082 - val_loss: 167.9798 - val_precision: 0.2632\n",
      "Epoch 23/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 211.6534 - precision: 0.3852 - val_loss: 181.6792 - val_precision: 0.2000\n",
      "Epoch 24/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 232.8437 - precision: 0.4013 - val_loss: 189.2511 - val_precision: 0.1667\n",
      "Epoch 25/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 236.6447 - precision: 0.3894 - val_loss: 201.4484 - val_precision: 0.0526\n",
      "Epoch 26/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 274.0326 - precision: 0.4143 - val_loss: 212.8432 - val_precision: 0.1500\n",
      "Epoch 27/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 276.7166 - precision: 0.3910 - val_loss: 215.3586 - val_precision: 0.2500\n",
      "Epoch 28/40\n",
      "121/121 [==============================] - 1s 12ms/step - loss: 288.1967 - precision: 0.4086 - val_loss: 227.5920 - val_precision: 0.1500\n",
      "Epoch 29/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 282.5406 - precision: 0.4028 - val_loss: 231.4408 - val_precision: 0.1667\n",
      "Epoch 30/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 290.5631 - precision: 0.4026 - val_loss: 239.3294 - val_precision: 0.2500\n",
      "Epoch 31/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 322.7168 - precision: 0.3947 - val_loss: 251.9925 - val_precision: 0.2632\n",
      "Epoch 32/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 343.4272 - precision: 0.3860 - val_loss: 266.9649 - val_precision: 0.2632\n",
      "Epoch 33/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 353.7883 - precision: 0.4058 - val_loss: 273.0307 - val_precision: 0.2500\n",
      "Epoch 34/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 327.9063 - precision: 0.4086 - val_loss: 275.3830 - val_precision: 0.1667\n",
      "Epoch 35/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 380.4243 - precision: 0.4006 - val_loss: 284.1656 - val_precision: 0.2500\n",
      "Epoch 36/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 421.7056 - precision: 0.4031 - val_loss: 299.7403 - val_precision: 0.1765\n",
      "Epoch 37/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 400.0579 - precision: 0.4046 - val_loss: 300.7274 - val_precision: 0.2000\n",
      "Epoch 38/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 412.2747 - precision: 0.3869 - val_loss: 308.3205 - val_precision: 0.2778\n",
      "Epoch 39/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 398.3323 - precision: 0.3986 - val_loss: 310.5465 - val_precision: 0.2632\n",
      "Epoch 40/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 467.9698 - precision: 0.3907 - val_loss: 322.5447 - val_precision: 0.3000\n",
      "Done.\n",
      "trainning lgbm\n",
      "We are trainning for this month: 06\n",
      "shape of the train set : (6118, 1663)\n",
      "shape of the test set : (73, 1663)\n",
      "hhere\n",
      "(73, 1664)\n",
      "(10, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (6037, 1618)\n",
      "shape of the y_train:  (6037, 14)\n",
      "shape of the X_test:  (10, 1618)\n",
      "shape of the y_test:  (10, 14)\n",
      "trainning deep learning\n",
      "Start training..\n",
      "\n",
      "Epoch 1/40\n",
      "121/121 [==============================] - 2s 9ms/step - loss: 9.8951 - precision: 0.3602 - val_loss: 13.3856 - val_precision: 0.5000\n",
      "Epoch 2/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 13.1519 - precision: 0.5239 - val_loss: 24.8368 - val_precision: 0.5000\n",
      "Epoch 3/40\n",
      "121/121 [==============================] - 1s 9ms/step - loss: 18.8847 - precision: 0.5115 - val_loss: 44.3669 - val_precision: 0.5000\n",
      "Epoch 4/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 25.1676 - precision: 0.4874 - val_loss: 70.4024 - val_precision: 0.6250\n",
      "Epoch 5/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 31.4805 - precision: 0.4673 - val_loss: 100.3036 - val_precision: 0.5000\n",
      "Epoch 6/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 37.1398 - precision: 0.4612 - val_loss: 135.4948 - val_precision: 0.3333\n",
      "Epoch 7/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 45.8013 - precision: 0.4737 - val_loss: 177.3795 - val_precision: 0.2222\n",
      "Epoch 8/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 51.4311 - precision: 0.4567 - val_loss: 221.1332 - val_precision: 0.3750\n",
      "Epoch 9/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 60.6277 - precision: 0.4516 - val_loss: 265.8301 - val_precision: 0.4286\n",
      "Epoch 10/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 67.8762 - precision: 0.4566 - val_loss: 315.8530 - val_precision: 0.4000\n",
      "Epoch 11/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 79.5662 - precision: 0.4472 - val_loss: 367.8996 - val_precision: 0.4000\n",
      "Epoch 12/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 87.0473 - precision: 0.4516 - val_loss: 426.9348 - val_precision: 0.3750\n",
      "Epoch 13/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 95.7434 - precision: 0.4407 - val_loss: 487.0743 - val_precision: 0.4444\n",
      "Epoch 14/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 107.3470 - precision: 0.4364 - val_loss: 556.9989 - val_precision: 0.2000\n",
      "Epoch 15/40\n",
      "121/121 [==============================] - 1s 10ms/step - loss: 117.0113 - precision: 0.4433 - val_loss: 621.9637 - val_precision: 0.3333\n",
      "Epoch 16/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 132.9956 - precision: 0.4388 - val_loss: 683.7690 - val_precision: 0.6667\n",
      "Epoch 17/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 150.1378 - precision: 0.4377 - val_loss: 758.3037 - val_precision: 0.5000\n",
      "Epoch 18/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 154.1510 - precision: 0.4260 - val_loss: 833.9482 - val_precision: 0.3333\n",
      "Epoch 19/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 160.9181 - precision: 0.4233 - val_loss: 913.8385 - val_precision: 0.5000\n",
      "Epoch 20/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 183.8324 - precision: 0.4309 - val_loss: 989.5554 - val_precision: 0.4444\n",
      "Epoch 21/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 187.3229 - precision: 0.4196 - val_loss: 1067.3151 - val_precision: 0.6000\n",
      "Epoch 22/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 207.7228 - precision: 0.4202 - val_loss: 1147.6830 - val_precision: 0.5556\n",
      "Epoch 23/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 204.5645 - precision: 0.4209 - val_loss: 1230.9387 - val_precision: 0.6667\n",
      "Epoch 24/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 221.9825 - precision: 0.4298 - val_loss: 1322.2913 - val_precision: 0.5556\n",
      "Epoch 25/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 223.7887 - precision: 0.4079 - val_loss: 1405.7400 - val_precision: 0.6667\n",
      "Epoch 26/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 233.8574 - precision: 0.4233 - val_loss: 1503.8362 - val_precision: 0.6000\n",
      "Epoch 27/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 262.1423 - precision: 0.4032 - val_loss: 1598.1573 - val_precision: 0.5556\n",
      "Epoch 28/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 273.0708 - precision: 0.4069 - val_loss: 1692.4192 - val_precision: 0.4000\n",
      "Epoch 29/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 283.9718 - precision: 0.4194 - val_loss: 1784.7188 - val_precision: 0.6000\n",
      "Epoch 30/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 312.6470 - precision: 0.4106 - val_loss: 1880.4156 - val_precision: 0.3000\n",
      "Epoch 31/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 297.6287 - precision: 0.4164 - val_loss: 1961.4062 - val_precision: 0.5000\n",
      "Epoch 32/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 299.4829 - precision: 0.4154 - val_loss: 2068.1069 - val_precision: 0.6000\n",
      "Epoch 33/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 341.5360 - precision: 0.4258 - val_loss: 2168.5005 - val_precision: 0.6250\n",
      "Epoch 34/40\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 359.6275 - precision: 0.4089 - val_loss: 2280.7329 - val_precision: 0.5556\n",
      "Epoch 35/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 352.2564 - precision: 0.4142 - val_loss: 2383.8440 - val_precision: 0.5000\n",
      "Epoch 36/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 377.9550 - precision: 0.4075 - val_loss: 2495.9553 - val_precision: 0.4000\n",
      "Epoch 37/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 416.8722 - precision: 0.4024 - val_loss: 2594.0415 - val_precision: 0.6000\n",
      "Epoch 38/40\n",
      "121/121 [==============================] - 1s 7ms/step - loss: 432.5506 - precision: 0.4111 - val_loss: 2699.8677 - val_precision: 0.4000\n",
      "Epoch 39/40\n",
      "121/121 [==============================] - 1s 8ms/step - loss: 419.0516 - precision: 0.4156 - val_loss: 2810.5295 - val_precision: 0.5000\n",
      "Epoch 40/40\n",
      "121/121 [==============================] - 1s 6ms/step - loss: 409.0508 - precision: 0.4037 - val_loss: 2923.5730 - val_precision: 0.5000\n",
      "Done.\n",
      "trainning lgbm\n",
      "We are trainning for this month: 07\n",
      "shape of the train set : (6191, 1663)\n",
      "shape of the test set : (83, 1663)\n",
      "hhere\n",
      "(83, 1664)\n",
      "(0, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (6047, 1618)\n",
      "shape of the y_train:  (6047, 14)\n",
      "ERROR Can't do this set\n",
      "trainning lgbm\n",
      "We are trainning for this month: 08\n",
      "shape of the train set : (6274, 1663)\n",
      "shape of the test set : (74, 1663)\n",
      "hhere\n",
      "(74, 1664)\n",
      "(0, 1663)\n",
      "We only keep 124 columns in totals\n",
      "shape of the x_train:  (6047, 1618)\n",
      "shape of the y_train:  (6047, 14)\n",
      "ERROR Can't do this set\n",
      "trainning lgbm\n"
     ]
    }
   ],
   "source": [
    "# all months we need to test so we train 7 times both models\n",
    "all_month = ['01', '02','03','05', '06','07', '08']\n",
    "\n",
    "\n",
    "for month in all_month : \n",
    "    print(f'We are trainning for this month: {month}')\n",
    "    display = False\n",
    "    save_model = True\n",
    "    A = prepare_and_split_data_placed(pd.read_hdf(f'../data/{month}_train_runners.h5','features'),\n",
    "                                                                          pd.read_hdf(f'../data/{month}_test_runners.h5','features'))\n",
    "    \n",
    "    if A!= False:\n",
    "        X_train, y_train, X_test, y_test, y_train_value, y_test_value, X_test_init = A\n",
    "        \n",
    "    #### DEEP LEARNING\n",
    "    print(\"trainning deep learning\")\n",
    "        \n",
    "        \n",
    "    #hyperparameters for the deep learning model\n",
    "    num_neutron = 96\n",
    "    batch_size = 50\n",
    "    epoch= 40\n",
    "    \n",
    "    model = train_dl(num_neutron,batch_size,epoch,X_train,y_train,X_test,y_test)\n",
    "    \n",
    "\n",
    "        if save_model:\n",
    "            model.save(f'model/placed_DL_{month}.h5')\n",
    "        \n",
    "    else :\n",
    "        pass\n",
    "\n",
    "        \n",
    "    #### LGBM\n",
    "        \n",
    "    print(\"trainning lgbm\")\n",
    "    new_columns = []\n",
    "    \n",
    "    ##We need to do this step to avoid multindexes to train with lgbm\n",
    "\n",
    "    for t in X_train.columns:\n",
    "        n = t[0]+ str(t[1])\n",
    "        new_columns.append(n)\n",
    "\n",
    "    new_columns_y = []\n",
    "\n",
    "    for t in y_train.columns:\n",
    "        n = t[0]+ str(t[1])\n",
    "        new_columns_y.append(n)\n",
    "\n",
    "    X_train.columns = new_columns\n",
    "    X_test.columns = new_columns\n",
    "\n",
    "    y_train.columns = new_columns_y\n",
    "    y_test.columns = new_columns_y\n",
    "\n",
    "    #classifier1 = LGBMClassifier(seed=80).fit(X_train, y_train_value)\n",
    "    #classifier1 = LGBMClassifier(**best_params).fit(X_train, y_train.values)\n",
    "    #classifier1 = MultiOutputClassifier(LGBMClassifier(**best_params)).fit(X_train, y_train.values)\n",
    "    \n",
    "    filename = f'model/placed_lgbm_{month}'\n",
    "    #save model\n",
    "    save_model = False\n",
    "    if save_model:\n",
    "        joblib.dump(classifier1, filename) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all months we need to test so we train 7 times both models\n",
    "all_month = ['01', '02','03','05', '06','07', '08']\n",
    "\n",
    "\n",
    "for month in all_month : \n",
    "    print(f'We are trainning for this month: {month}')\n",
    "    display = False\n",
    "    save_model = True\n",
    "    A = prepare_and_split_data_placed(pd.read_hdf(f'../data/{month}_train_runners.h5','features'),\n",
    "                                                                          pd.read_hdf(f'../data/{month}_test_runners.h5','features'))\n",
    "    \n",
    "    if A!= False:\n",
    "        X_train, y_train, X_test, y_test, y_train_value, y_test_value, X_test_init = A\n",
    "        \n",
    "    #### DEEP LEARNING\n",
    "    print(\"trainning deep learning\")\n",
    "        \n",
    "        \n",
    "    #hyperparameters for the deep learning model\n",
    "    num_neutron = 96\n",
    "    batch_size = 50\n",
    "    epoch= 40\n",
    "    \n",
    "    model = train_dl(num_neutron,batch_size,epoch,X_train,y_train,X_test,y_test)\n",
    "    \n",
    "\n",
    "        if save_model:\n",
    "            model.save(f'model/placed_DL_{month}.h5')\n",
    "        \n",
    "    else :\n",
    "        pass\n",
    "\n",
    "        \n",
    "    #### LGBM\n",
    "        \n",
    "    print(\"trainning lgbm\")\n",
    "    new_columns = []\n",
    "    \n",
    "    ##We need to do this step to avoid multindexes to train with lgbm\n",
    "\n",
    "    for t in X_train.columns:\n",
    "        n = t[0]+ str(t[1])\n",
    "        new_columns.append(n)\n",
    "\n",
    "    new_columns_y = []\n",
    "\n",
    "    for t in y_train.columns:\n",
    "        n = t[0]+ str(t[1])\n",
    "        new_columns_y.append(n)\n",
    "\n",
    "    X_train.columns = new_columns\n",
    "    X_test.columns = new_columns\n",
    "\n",
    "    y_train.columns = new_columns_y\n",
    "    y_test.columns = new_columns_y\n",
    "\n",
    "    #classifier1 = LGBMClassifier(seed=80).fit(X_train, y_train_value)\n",
    "    #classifier1 = LGBMClassifier(**best_params).fit(X_train, y_train.values)\n",
    "    #classifier1 = MultiOutputClassifier(LGBMClassifier(**best_params)).fit(X_train, y_train.values)\n",
    "    \n",
    "    filename = f'model/placed_lgbm_{month}'\n",
    "    #save model\n",
    "    save_model = False\n",
    "    if save_model:\n",
    "        joblib.dump(classifier1, filename) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856add0a",
   "metadata": {},
   "source": [
    "# It takes 35 min to train all this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72b96b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LGBMClassifier(boosting='gbdt',\n",
       "                                               feature_fraction=0.6,\n",
       "                                               learning_rate=0.03, max_depth=9,\n",
       "                                               metric='multi_error',\n",
       "                                               min_child_samples=90,\n",
       "                                               min_data_in_leaf=99,\n",
       "                                               n_estimators=300, num_class=14,\n",
       "                                               num_leaves=50,\n",
       "                                               objective='multiclass',\n",
       "                                               reg_alpha=0.31, reg_lambda=0.68,\n",
       "                                               seed=80, subsample=0.88,\n",
       "                                               verbose=-1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cf4c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgbm= classifier1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7b0a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_lgbm)\n",
    "len(y_pred_lgbm[0])\n",
    "len(y_pred_lgbm[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cee88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
